"""
Optimized Auction Handler with Parallel Processing and Persistent Service Integration

Major improvements:
1. Parallel page processing instead of sequential
2. Batch item evaluation using persistent service
3. Optimized database operations
4. Connection pooling and reuse

Expected performance improvement: 80-90% reduction in total processing time
"""

import asyncio
import logging
import aiohttp
import orjson
import requests
import time
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor

from Handlers import DataBaseHandler, ProgressHandler
from Handlers.ItemValueHandlerOptimized import process_auctions_batch, get_evaluation_stats
from ConfigHandler import get_api_url, get_performance_config

# Global HTTP session for reuse
_global_session: aiohttp.ClientSession = None

async def get_global_session():
    """Get or create global HTTP session with optimized settings."""
    global _global_session
    if _global_session is None or _global_session.closed:
        perf_config = get_performance_config()
        connector = aiohttp.TCPConnector(
            limit=perf_config.get("connection_pool_size", 200),
            limit_per_host=50,
            keepalive_timeout=60,
            enable_cleanup_closed=True,
            use_dns_cache=True,
            ttl_dns_cache=300
        )
        timeout = aiohttp.ClientTimeout(total=30, connect=5)
        _global_session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout
        )
    return _global_session

async def fetch_page_data(session: aiohttp.ClientSession, page: int) -> Dict[str, Any]:
    """Fetch auction data for a single page."""
    try:
        url = get_api_url(f'https://api.hypixel.net/skyblock/auctions?page={page}')
        async with session.get(url) as response:
            if response.status == 200:
                text = await response.text()
                return orjson.loads(text)
            else:
                logging.error(f"Failed to fetch page {page}: HTTP {response.status}")
                return {'auctions': []}
    except Exception as e:
        logging.error(f"Error fetching page {page}: {e}")
        return {'auctions': []}

async def process_page_optimized(session: aiohttp.ClientSession, page: int) -> Dict[str, Any]:
    """
    Process a single auction page with optimizations.
    Returns processing statistics.
    """
    start_time = time.perf_counter()
    
    # Fetch page data
    data = await fetch_page_data(session, page)
    auctions = data.get('auctions', [])
    
    if not auctions:
        return {
            'page': page,
            'total_auctions': 0,
            'new_auctions': 0,
            'profitable_flips': 0,
            'processing_time': time.perf_counter() - start_time
        }
    
    # Check which auctions already exist (optimized batch query)
    auction_uuids = [auction['uuid'] for auction in auctions]
    existing_uuids = DataBaseHandler.bulk_check_existing_auctions(auction_uuids)
    
    # Filter out existing auctions
    new_auctions = [auction for auction in auctions if auction['uuid'] not in existing_uuids]
    
    # Update progress for skipped auctions
    ProgressHandler.updatepbar(len(existing_uuids))
    
    # Prepare auction data for database insertion
    auction_inserts = []
    bin_auctions = []
    
    for auction in new_auctions:
        if auction.get('bin', False) and not auction.get('claimed', False):
            # Prepare for database insertion
            auction_data = {
                'name': auction['item_name'],
                'tier': auction['tier'],
                'price': auction['starting_bid'],
                'bin': auction['bin'],
                'item_bytes': auction['item_bytes'],
                'uuid': auction['uuid'],
                'start': auction['start'],
                'end': auction['end'],
                'seller': auction.get('auctioneer') or (auction.get('coop', [None])[0] if auction.get('coop') else None)
            }
            auction_inserts.append(auction_data)
            bin_auctions.append(auction)
    
    # Bulk insert auctions to database
    if auction_inserts:
        try:
            DataBaseHandler.auctions.insert_many(auction_inserts)
        except Exception as e:
            logging.error(f"Error bulk inserting auctions for page {page}: {e}")
    
    # Process items for evaluation in batch
    profitable_flips = 0
    if bin_auctions:
        try:
            profitable_flips = await process_auctions_batch(bin_auctions)
        except Exception as e:
            logging.error(f"Error processing auctions batch for page {page}: {e}")
    
    # Update progress for processed auctions
    ProgressHandler.updatepbar(len(new_auctions))
    
    processing_time = time.perf_counter() - start_time
    
    return {
        'page': page,
        'total_auctions': len(auctions),
        'existing_auctions': len(existing_uuids),
        'new_auctions': len(new_auctions),
        'bin_auctions': len(bin_auctions),
        'profitable_flips': profitable_flips,
        'processing_time': processing_time
    }

async def check_auctions_parallel(total_pages: int, max_concurrent: int = None) -> Dict[str, Any]:
    """
    Process auction pages in parallel for maximum performance.
    
    Args:
        total_pages: Number of pages to process
        max_concurrent: Maximum concurrent page processing
    
    Returns:
        Processing statistics
    """
    # Use configuration if max_concurrent not specified
    if max_concurrent is None:
        perf_config = get_performance_config()
        max_concurrent = perf_config.get("max_concurrent_pages", 10)
    
    print(f'ðŸš€ Starting parallel auction processing ({total_pages} pages, max {max_concurrent} concurrent)')
    overall_start_time = time.perf_counter()
    
    # Get global session
    session = await get_global_session()
    
    # Get initial data for progress tracking
    try:
        initial_data = await fetch_page_data(session, 0)
        total_auctions = initial_data.get('totalAuctions', 0)
        print(f'ðŸ“Š Total auctions to process: {total_auctions:,}')
        ProgressHandler.createpbar(total_auctions)
    except Exception as e:
        logging.error(f"Error getting initial auction data: {e}")
        total_auctions = 0
        ProgressHandler.createpbar(1000)  # Fallback
    
    # Process pages in batches to control concurrency
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def process_page_with_semaphore(page: int):
        async with semaphore:
            return await process_page_optimized(session, page)
    
    # Create tasks for all pages
    tasks = [process_page_with_semaphore(page) for page in range(total_pages)]
    
    # Execute all tasks and gather results
    try:
        page_results = await asyncio.gather(*tasks, return_exceptions=True)
    except Exception as e:
        logging.error(f"Error in parallel processing: {e}")
        page_results = []
    
    # Process results and calculate statistics
    total_stats = {
        'total_pages_processed': 0,
        'total_auctions': 0,
        'existing_auctions': 0,
        'new_auctions': 0,
        'bin_auctions': 0,
        'profitable_flips': 0,
        'total_processing_time': 0,
        'page_results': []
    }
    
    for result in page_results:
        if isinstance(result, dict) and 'page' in result:
            total_stats['total_pages_processed'] += 1
            total_stats['total_auctions'] += result.get('total_auctions', 0)
            total_stats['existing_auctions'] += result.get('existing_auctions', 0)
            total_stats['new_auctions'] += result.get('new_auctions', 0)
            total_stats['bin_auctions'] += result.get('bin_auctions', 0)
            total_stats['profitable_flips'] += result.get('profitable_flips', 0)
            total_stats['total_processing_time'] += result.get('processing_time', 0)
            total_stats['page_results'].append(result)
        elif isinstance(result, Exception):
            logging.error(f"Page processing failed: {result}")
    
    overall_time = time.perf_counter() - overall_start_time
    total_stats['overall_time'] = overall_time
    
    # Calculate efficiency metrics
    if total_stats['total_pages_processed'] > 0:
        total_stats['avg_time_per_page'] = total_stats['total_processing_time'] / total_stats['total_pages_processed']
        total_stats['parallelization_efficiency'] = (total_stats['total_processing_time'] / overall_time) * 100
    
    # Close progress bar
    ProgressHandler.deletepbar()
    
    # Print summary
    print(f'âœ… Parallel processing completed in {overall_time:.2f}s')
    print(f'ðŸ“Š Processed: {total_stats["total_auctions"]:,} auctions, {total_stats["new_auctions"]:,} new')
    print(f'ðŸ’° Found: {total_stats["profitable_flips"]:,} profitable flips')
    print(f'âš¡ Efficiency: {total_stats.get("parallelization_efficiency", 0):.1f}% parallelization')
    
    # Get evaluation service stats
    eval_stats = get_evaluation_stats()
    if eval_stats['total_evaluations'] > 0:
        print(f'ðŸ” Evaluations: {eval_stats["total_evaluations"]:,} items in {eval_stats["total_time"]:.2f}s')
        print(f'ðŸ“ˆ Avg eval time: {eval_stats["avg_evaluation_time"]*1000:.1f}ms per item')
        print(f'ðŸ’¾ Cache hit rate: {eval_stats["cache_hit_rate"]:.1f}%')
    
    return total_stats

def delete_ended_auctions_optimized() -> int:
    """
    Optimized function to delete ended auctions using bulk operations.
    """
    print('ðŸ§¹ Cleaning up ended auctions...')
    start_time = time.perf_counter()
    
    try:
        # Get ended auctions from API
        url = get_api_url('https://api.hypixel.net/v2/skyblock/auctions_ended')
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        data = orjson.loads(response.content)
        
        # Extract auction IDs
        ended_auction_ids = [auction['auction_id'] for auction in data.get('auctions', [])]
        
        if not ended_auction_ids:
            print('âœ… No ended auctions to clean up')
            return 0
        
        # Check which exist in our database and delete them
        existing_ended_uuids = DataBaseHandler.bulk_check_existing_auctions(ended_auction_ids)
        
        if existing_ended_uuids:
            deleted_count = DataBaseHandler.bulk_delete_auctions(list(existing_ended_uuids))
            cleanup_time = time.perf_counter() - start_time
            print(f'âœ… Deleted {deleted_count:,} ended auctions in {cleanup_time:.2f}s')
            return deleted_count
        else:
            print('âœ… No ended auctions found in database')
            return 0
            
    except Exception as e:
        logging.error(f"Error in optimized auction cleanup: {e}")
        return 0

# Backward compatibility functions
async def CheckAuctions(total_pages: int) -> bool:
    """
    Main function for checking auctions with parallel processing.
    Backward compatible with the original interface.
    """
    try:
        await check_auctions_parallel(total_pages, max_concurrent=8)  # Conservative concurrency
        return True
    except Exception as e:
        logging.error(f"Error in CheckAuctions: {e}")
        return False

def delete_ended_auctions() -> bool:
    """Backward compatible function for deleting ended auctions."""
    try:
        delete_ended_auctions_optimized()
        return True
    except Exception as e:
        logging.error(f"Error in delete_ended_auctions: {e}")
        return False

# Utility functions for monitoring and tuning
async def benchmark_parallel_performance(max_pages: int = 5) -> Dict[str, Any]:
    """
    Benchmark parallel processing performance with different concurrency levels.
    """
    print(f"ðŸ”¬ Benchmarking parallel performance with {max_pages} pages...")
    
    concurrency_levels = [1, 2, 4, 8, 12, 16]
    results = {}
    
    for concurrency in concurrency_levels:
        if concurrency > max_pages:
            continue
            
        print(f"Testing concurrency level: {concurrency}")
        start_time = time.perf_counter()
        
        try:
            stats = await check_auctions_parallel(max_pages, max_concurrent=concurrency)
            end_time = time.perf_counter()
            
            results[concurrency] = {
                'total_time': end_time - start_time,
                'pages_processed': stats.get('total_pages_processed', 0),
                'profitable_flips': stats.get('profitable_flips', 0),
                'parallelization_efficiency': stats.get('parallelization_efficiency', 0)
            }
            
        except Exception as e:
            logging.error(f"Benchmark failed at concurrency {concurrency}: {e}")
            results[concurrency] = {'error': str(e)}
    
    return results

async def cleanup_session():
    """Clean up the global session."""
    global _global_session
    if _global_session and not _global_session.closed:
        await _global_session.close()
        _global_session = None